{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3409aa35",
   "metadata": {},
   "source": [
    "# ðŸ° H-MOLQD: Hybrid Masked-diffusion Optimization with Logical Quality Diversity\n",
    "\n",
    "**GPU-Accelerated Training Notebook for Zelda Dungeon Generation**\n",
    "\n",
    "## Architecture Overview (6 Blocks)\n",
    "\n",
    "```\n",
    "VGLC Data (18 dungeons Ã— .txt grids + .dot mission graphs)\n",
    "    â”‚\n",
    "    â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Block I  â€” VQ-VAE: 44 tiles â†’ 512 codebook â†’ latent_dim=64       â”‚\n",
    "â”‚  Block II â€” Dual-Stream Condition Encoder (GATv2Conv GNN + Local)  â”‚\n",
    "â”‚  Block IIIâ€” Latent Diffusion (U-Net, cosine schedule, DDIM)        â”‚\n",
    "â”‚  Block IV â€” LogicNet (differentiable solvability + key-lock check) â”‚\n",
    "â”‚  Block V  â€” WFC Refiner (inference-only post-processing)           â”‚\n",
    "â”‚  Block VI â€” Cognitive Validator (A* solver + CBS agent)            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## Training Pipeline\n",
    "- **Stage 1**: VQ-VAE pretraining (reconstructing dungeon grids)\n",
    "- **Stage 2**: Latent Diffusion with **real .dot graph conditioning** + LogicNet guidance\n",
    "\n",
    "## How to Use on Kaggle\n",
    "1. Upload your project repo as a **Kaggle Dataset**\n",
    "2. Enable **GPU accelerator** (T4/P100)\n",
    "3. Run all cells sequentially\n",
    "4. Download checkpoints from outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc616524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 1: Environment Setup\n",
    "# ============================================================================\n",
    "import os, sys, subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "IS_KAGGLE = os.path.exists('/kaggle/working')\n",
    "IS_COLAB = 'google.colab' in sys.modules\n",
    "ENV_NAME = \"Kaggle\" if IS_KAGGLE else (\"Colab\" if IS_COLAB else \"Local\")\n",
    "print(f\"ðŸ–¥ï¸  Environment: {ENV_NAME}\")\n",
    "\n",
    "# --- Find project root ---\n",
    "if IS_KAGGLE:\n",
    "    WORKING_DIR = Path('/kaggle/working')\n",
    "    candidates = [\n",
    "        Path('/kaggle/input/kltn'), Path('/kaggle/input/hmolqd'),\n",
    "        Path('/kaggle/input/kltn/KLTN'), Path('/kaggle/working/KLTN'),\n",
    "    ]\n",
    "elif IS_COLAB:\n",
    "    WORKING_DIR = Path('/content')\n",
    "    candidates = [Path('/content/KLTN'), Path('/content/drive/MyDrive/KLTN')]\n",
    "else:\n",
    "    WORKING_DIR = Path('.').resolve()\n",
    "    candidates = [Path('.').resolve(), Path('..').resolve()]\n",
    "\n",
    "PROJECT_ROOT = None\n",
    "for c in candidates:\n",
    "    if (c / 'src' / 'train_vqvae.py').exists():\n",
    "        PROJECT_ROOT = c\n",
    "        break\n",
    "\n",
    "if PROJECT_ROOT is None and not (not IS_KAGGLE and not IS_COLAB):\n",
    "    clone_target = WORKING_DIR / 'KLTN'\n",
    "    if not clone_target.exists():\n",
    "        subprocess.run(['git', 'clone', 'https://github.com/YOUR_USERNAME/KLTN.git',\n",
    "                        str(clone_target)], check=False)\n",
    "    if (clone_target / 'src' / 'train_vqvae.py').exists():\n",
    "        PROJECT_ROOT = clone_target\n",
    "\n",
    "if PROJECT_ROOT is None:\n",
    "    raise FileNotFoundError(\"âŒ Could not find project root! Upload repo as Kaggle dataset.\")\n",
    "\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "CHECKPOINT_DIR = (WORKING_DIR if IS_KAGGLE else PROJECT_ROOT) / 'checkpoints'\n",
    "OUTPUT_DIR = (WORKING_DIR if IS_KAGGLE else PROJECT_ROOT) / 'output'\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / 'data' / 'The Legend of Zelda'\n",
    "if not DATA_DIR.exists():\n",
    "    DATA_DIR = PROJECT_ROOT / 'Data' / 'The Legend of Zelda'\n",
    "\n",
    "print(f\"âœ… Project root: {PROJECT_ROOT}\")\n",
    "print(f\"ðŸ“ Data dir: {DATA_DIR} (exists={DATA_DIR.exists()})\")\n",
    "print(f\"ðŸ’¾ Checkpoints: {CHECKPOINT_DIR}\")\n",
    "\n",
    "# Install dependencies\n",
    "req = PROJECT_ROOT / 'requirements-hmolqd.txt'\n",
    "if req.exists():\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', '-r', str(req)], check=False)\n",
    "print(\"ðŸŽ‰ Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f07e79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: Import Verification & GPU Check\n",
    "# ============================================================================\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(levelname)s | %(message)s', datefmt='%H:%M:%S')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} ({torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB)\")\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "# Import project modules\n",
    "from src.core.vqvae import SemanticVQVAE, create_vqvae, VQVAETrainer\n",
    "from src.core.latent_diffusion import create_latent_diffusion\n",
    "from src.core.condition_encoder import create_condition_encoder\n",
    "from src.core.logic_net import LogicNet\n",
    "from src.data.zelda_loader import create_dataloader, graph_collate_fn\n",
    "from src.train_vqvae import grids_to_onehot\n",
    "from src.train_diffusion import DiffusionTrainingConfig, DiffusionTrainer\n",
    "print(\"âœ… All modules imported\")\n",
    "\n",
    "# Quick data test\n",
    "loader_test = create_dataloader(str(DATA_DIR), batch_size=2, use_vglc=True, normalize=True, load_graphs=True)\n",
    "print(f\"\\nðŸ“Š Dataset: {len(loader_test.dataset)} dungeons\")\n",
    "for batch in loader_test:\n",
    "    if isinstance(batch, (list, tuple)):\n",
    "        imgs, graphs = batch\n",
    "        g = graphs[0]\n",
    "        print(f\"   Image batch: {imgs.shape}\")\n",
    "        print(f\"   Graph[0]: {g['num_nodes']} nodes, {g['num_edges']} edges, features={g['node_features'].shape}\")\n",
    "    break\n",
    "print(\"âœ… Data loading verified with real .dot graphs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aed0790",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ”µ Stage 1: VQ-VAE Pretraining (Block I)\n",
    "\n",
    "Train the Semantic VQ-VAE to reconstruct dungeon grids.\n",
    "- **Input**: 44-class one-hot tiles `[B, 44, H, W]`\n",
    "- **Codebook**: 512 embeddings, latent_dim=64\n",
    "- **Goal**: â‰¥85% reconstruction accuracy before Stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f582648a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: Stage 1 â€” VQ-VAE Pretraining\n",
    "# ============================================================================\n",
    "import json, time\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "\n",
    "VQVAE_EPOCHS = 50          # Increase to 200-300 for production\n",
    "VQVAE_BATCH_SIZE = 2\n",
    "VQVAE_LR = 3e-4\n",
    "MIN_SAMPLES_PER_EPOCH = 64\n",
    "VQVAE_SAVE_PATH = CHECKPOINT_DIR / 'vqvae_pretrained.pth'\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ðŸ”µ STAGE 1: VQ-VAE PRETRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check for existing checkpoint\n",
    "SKIP_VQVAE = False\n",
    "if VQVAE_SAVE_PATH.exists():\n",
    "    ckpt = torch.load(VQVAE_SAVE_PATH, map_location='cpu', weights_only=False)\n",
    "    existing_acc = ckpt.get('accuracy', 0)\n",
    "    print(f\"âš¡ Found checkpoint: epoch={ckpt.get('epoch', 0)+1}, accuracy={existing_acc:.3f}\")\n",
    "    if existing_acc >= 0.85:\n",
    "        print(\"   âœ… Accuracy â‰¥ 85% â€” skipping (delete checkpoint to retrain)\")\n",
    "        SKIP_VQVAE = True\n",
    "\n",
    "if not SKIP_VQVAE:\n",
    "    # Dataset with upsampling\n",
    "    base_loader = create_dataloader(str(DATA_DIR), batch_size=VQVAE_BATCH_SIZE,\n",
    "                                     shuffle=True, use_vglc=True, normalize=True, load_graphs=False)\n",
    "    dataset = base_loader.dataset\n",
    "    sampler = RandomSampler(dataset, replacement=True, num_samples=max(len(dataset), MIN_SAMPLES_PER_EPOCH))\n",
    "    dataloader = DataLoader(dataset, batch_size=VQVAE_BATCH_SIZE, sampler=sampler, num_workers=0, drop_last=True)\n",
    "    print(f\"ðŸ“Š {len(dataset)} dungeons, {len(dataloader)} batches/epoch (upsampled)\")\n",
    "\n",
    "    # Model\n",
    "    vqvae_model = create_vqvae(num_classes=44, codebook_size=512, latent_dim=64).to(DEVICE)\n",
    "    trainer = VQVAETrainer(vqvae_model, lr=VQVAE_LR)\n",
    "    print(f\"ðŸ—ï¸  VQ-VAE: {sum(p.numel() for p in vqvae_model.parameters()):,} params\")\n",
    "\n",
    "    best_loss, history = float('inf'), []\n",
    "    t0 = time.time()\n",
    "\n",
    "    for epoch in range(VQVAE_EPOCHS):\n",
    "        vqvae_model.train()\n",
    "        metrics_sum = {'loss': 0, 'recon_loss': 0, 'vq_loss': 0, 'perplexity': 0}\n",
    "        nb = 0\n",
    "        for batch in dataloader:\n",
    "            if isinstance(batch, (list, tuple)): batch = batch[0]\n",
    "            batch = batch.to(DEVICE)\n",
    "            x_onehot = grids_to_onehot(batch, num_classes=44)\n",
    "            _, m = trainer.train_step(x_onehot)\n",
    "            for k in metrics_sum: metrics_sum[k] += m.get(k, 0.0)\n",
    "            nb += 1\n",
    "        for k in metrics_sum: metrics_sum[k] /= max(nb, 1)\n",
    "\n",
    "        # Eval accuracy\n",
    "        vqvae_model.eval()\n",
    "        acc_sum, acc_n = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                if isinstance(batch, (list, tuple)): batch = batch[0]\n",
    "                x = grids_to_onehot(batch.to(DEVICE), num_classes=44)\n",
    "                info = trainer.eval_step(x)\n",
    "                acc_sum += info['accuracy']; acc_n += 1\n",
    "                if acc_n >= 5: break\n",
    "        eval_acc = acc_sum / max(acc_n, 1)\n",
    "        metrics_sum['accuracy'] = eval_acc\n",
    "        history.append({'epoch': epoch+1, **metrics_sum})\n",
    "\n",
    "        if metrics_sum['loss'] < best_loss:\n",
    "            best_loss = metrics_sum['loss']\n",
    "            torch.save({'epoch': epoch, 'model_state_dict': vqvae_model.state_dict(),\n",
    "                        'optimizer_state_dict': trainer.optimizer.state_dict(),\n",
    "                        'loss': best_loss, 'accuracy': eval_acc,\n",
    "                        'perplexity': metrics_sum['perplexity']}, VQVAE_SAVE_PATH)\n",
    "\n",
    "        if (epoch+1) % 5 == 0 or epoch == 0:\n",
    "            print(f\"  Epoch {epoch+1:3d}/{VQVAE_EPOCHS} | loss={metrics_sum['loss']:.4f} | \"\n",
    "                  f\"recon={metrics_sum['recon_loss']:.4f} | accuracy={eval_acc:.3f} | \"\n",
    "                  f\"{time.time()-t0:.0f}s\")\n",
    "\n",
    "    print(f\"\\nâœ… VQ-VAE done! Best loss={best_loss:.4f}, Final acc={eval_acc:.3f}\")\n",
    "    print(f\"   Saved to: {VQVAE_SAVE_PATH}\")\n",
    "\n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    ep_x = [h['epoch'] for h in history]\n",
    "    axes[0].plot(ep_x, [h['loss'] for h in history], 'b-'); axes[0].set_title('Loss'); axes[0].grid(True, alpha=0.3)\n",
    "    axes[1].plot(ep_x, [h['recon_loss'] for h in history], 'r-', label='Recon')\n",
    "    axes[1].plot(ep_x, [h['vq_loss'] for h in history], 'g-', label='VQ')\n",
    "    axes[1].set_title('Components'); axes[1].legend(); axes[1].grid(True, alpha=0.3)\n",
    "    axes[2].plot(ep_x, [h['accuracy'] for h in history], 'm-'); axes[2].set_title('Accuracy')\n",
    "    axes[2].set_ylim(0, 1); axes[2].grid(True, alpha=0.3)\n",
    "    plt.suptitle('Stage 1: VQ-VAE Training', fontweight='bold')\n",
    "    plt.tight_layout(); plt.savefig(str(OUTPUT_DIR / 'vqvae_curves.png'), dpi=150); plt.show()\n",
    "else:\n",
    "    print(\"â­ï¸  Skipping VQ-VAE training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010b1ec6",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŸ¢ Stage 2: Latent Diffusion with Real Graph Conditioning\n",
    "\n",
    "Full pipeline training using **real .dot mission graphs** from VGLC:\n",
    "- **U-Net** denoiser in VQ-VAE latent space\n",
    "- **GATv2Conv GNN** encodes real dungeon graph topology (nodes=rooms, edges=doors)\n",
    "- **LogicNet** gradient guidance: differentiable solvability + key-lock checking\n",
    "- **EMA** model weights for stable sampling\n",
    "\n",
    "| Phase | Epochs | Loss Components |\n",
    "|-------|--------|-----------------|\n",
    "| Warmup | 1â€“5 | Diffusion only (no logic) |\n",
    "| Full | 6+ | Diffusion + Î±Ã—LogicNet |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b867f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 4: Stage 2 â€” Diffusion Training with Real Graph Conditioning\n",
    "# ============================================================================\n",
    "import time\n",
    "\n",
    "DIFFUSION_EPOCHS = 500       # 100+ recommended; 500 for production\n",
    "DIFFUSION_BATCH_SIZE = 2\n",
    "DIFFUSION_LR = 1e-4\n",
    "ALPHA_LOGIC = 0.1\n",
    "WARMUP_EPOCHS = 5\n",
    "SAVE_EVERY = 10\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ðŸŸ¢ STAGE 2: LATENT DIFFUSION TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if not VQVAE_SAVE_PATH.exists():\n",
    "    raise FileNotFoundError(f\"âŒ VQ-VAE checkpoint not found. Run Stage 1 first!\")\n",
    "\n",
    "config = DiffusionTrainingConfig(\n",
    "    data_dir=str(DATA_DIR), batch_size=DIFFUSION_BATCH_SIZE, use_vglc=True,\n",
    "    vqvae_checkpoint=str(VQVAE_SAVE_PATH), latent_dim=64, model_channels=128,\n",
    "    context_dim=256, num_timesteps=1000, schedule_type='cosine',\n",
    "    num_logic_iterations=30, guidance_scale=1.0, epochs=DIFFUSION_EPOCHS,\n",
    "    learning_rate=DIFFUSION_LR, alpha_visual=1.0, alpha_logic=ALPHA_LOGIC,\n",
    "    warmup_epochs=WARMUP_EPOCHS, checkpoint_dir=str(CHECKPOINT_DIR),\n",
    "    save_every=SAVE_EVERY, device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    ")\n",
    "\n",
    "# Data loaders with REAL graph data from .dot files\n",
    "train_loader = create_dataloader(config.data_dir, batch_size=config.batch_size,\n",
    "    shuffle=True, use_vglc=True, normalize=True, load_graphs=True)\n",
    "val_loader = create_dataloader(config.data_dir, batch_size=config.batch_size,\n",
    "    shuffle=False, use_vglc=True, normalize=True, load_graphs=True)\n",
    "print(f\"ðŸ“Š {len(train_loader.dataset)} dungeons with real .dot graphs, {len(train_loader)} batches/epoch\")\n",
    "\n",
    "# Create trainer\n",
    "diff_trainer = DiffusionTrainer(config)\n",
    "print(f\"ðŸ—ï¸  All models on {config.device}, VQ-VAE frozen âœ…\")\n",
    "\n",
    "# Resume from checkpoint if exists\n",
    "resume_path = CHECKPOINT_DIR / 'final_model.pth'\n",
    "start_epoch = 0\n",
    "if resume_path.exists():\n",
    "    try:\n",
    "        diff_trainer.load_checkpoint(str(resume_path))\n",
    "        start_epoch = diff_trainer.epoch\n",
    "        print(f\"âš¡ Resumed from epoch {start_epoch}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Resume failed ({e}), starting fresh\")\n",
    "\n",
    "# Training loop\n",
    "best_solv, history = 0.0, []\n",
    "t0 = time.time()\n",
    "print(f\"\\n{'Epoch':>6} | {'Loss':>8} | {'Diffusion':>10} | {'Logic':>8} | {'Val Solv':>10} | {'Time':>6}\")\n",
    "print(\"â”€\" * 65)\n",
    "\n",
    "for epoch in range(start_epoch, DIFFUSION_EPOCHS):\n",
    "    train_m = diff_trainer.train_epoch(train_loader)\n",
    "    val_m = diff_trainer.validate(val_loader, num_samples=4)\n",
    "    lr = diff_trainer.scheduler.get_last_lr()[0]\n",
    "\n",
    "    rec = {'epoch': epoch+1, **train_m, 'val_solvability': val_m['val_solvability'], 'lr': lr}\n",
    "    history.append(rec)\n",
    "\n",
    "    logic_flag = \"ðŸ”’\" if epoch < WARMUP_EPOCHS else \"âœ…\"\n",
    "    elapsed = time.time() - t0\n",
    "    print(f\"  {epoch+1:4d}   | {train_m['loss']:.4f}  | {train_m['diffusion_loss']:.6f}  | \"\n",
    "          f\"{train_m['logic_loss']:.4f}{logic_flag} | {val_m['val_solvability']:.4f}     | {elapsed/60:.1f}m\")\n",
    "\n",
    "    if (epoch+1) % SAVE_EVERY == 0:\n",
    "        diff_trainer.save_checkpoint(str(CHECKPOINT_DIR / f'checkpoint_{epoch+1:04d}.pth'), rec)\n",
    "    if val_m['val_solvability'] > best_solv:\n",
    "        best_solv = val_m['val_solvability']\n",
    "        diff_trainer.save_checkpoint(str(CHECKPOINT_DIR / 'best_model.pth'), rec)\n",
    "\n",
    "diff_trainer.save_checkpoint(str(CHECKPOINT_DIR / 'final_model.pth'), history[-1] if history else None)\n",
    "print(f\"\\nâœ… Done! Best val solvability: {best_solv:.4f}, Time: {(time.time()-t0)/60:.1f} min\")\n",
    "\n",
    "with open(CHECKPOINT_DIR / 'diffusion_history.json', 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "# Plot\n",
    "if len(history) > 1:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    ep_x = [h['epoch'] for h in history]\n",
    "    axes[0,0].plot(ep_x, [h['loss'] for h in history], 'b-'); axes[0,0].set_title('Total Loss'); axes[0,0].grid(True, alpha=0.3)\n",
    "    axes[0,1].plot(ep_x, [h['diffusion_loss'] for h in history], 'r-', label='Diffusion')\n",
    "    axes[0,1].plot(ep_x, [h['logic_loss'] for h in history], 'g-', label='Logic')\n",
    "    axes[0,1].axvline(x=WARMUP_EPOCHS, color='gray', ls='--', alpha=0.5, label='Warmup end')\n",
    "    axes[0,1].set_title('Loss Components'); axes[0,1].legend(); axes[0,1].grid(True, alpha=0.3)\n",
    "    axes[1,0].plot(ep_x, [h['solvability'] for h in history], 'c-', label='Train')\n",
    "    axes[1,0].plot(ep_x, [h['val_solvability'] for h in history], 'm-', label='Val')\n",
    "    axes[1,0].set_title('Solvability'); axes[1,0].legend(); axes[1,0].set_ylim(0,1); axes[1,0].grid(True, alpha=0.3)\n",
    "    axes[1,1].plot(ep_x, [h['lr'] for h in history], 'k-'); axes[1,1].set_title('Learning Rate')\n",
    "    axes[1,1].set_yscale('log'); axes[1,1].grid(True, alpha=0.3)\n",
    "    plt.suptitle('Stage 2: Diffusion Training', fontweight='bold')\n",
    "    plt.tight_layout(); plt.savefig(str(OUTPUT_DIR / 'diffusion_curves.png'), dpi=150); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba1a304",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ¨ Generation & Visualization\n",
    "\n",
    "Generate sample dungeons using the trained model with graph-conditioned DDIM sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32a094e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 5: Generate Sample Dungeons\n",
    "# ============================================================================\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "NUM_SAMPLES = 4\n",
    "\n",
    "print(\"ðŸŽ¨ GENERATING SAMPLE DUNGEONS\")\n",
    "\n",
    "# Load best checkpoint\n",
    "ckpt_path = CHECKPOINT_DIR / 'best_model.pth'\n",
    "if not ckpt_path.exists():\n",
    "    ckpt_path = CHECKPOINT_DIR / 'final_model.pth'\n",
    "\n",
    "gen_config = DiffusionTrainingConfig(\n",
    "    data_dir=str(DATA_DIR), batch_size=1, use_vglc=True,\n",
    "    vqvae_checkpoint=str(VQVAE_SAVE_PATH), latent_dim=64,\n",
    "    model_channels=128, context_dim=256, num_timesteps=1000,\n",
    "    schedule_type='cosine', device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    ")\n",
    "gen = DiffusionTrainer(gen_config)\n",
    "gen.load_checkpoint(str(ckpt_path))\n",
    "print(f\"âœ… Loaded model (epoch {gen.epoch})\")\n",
    "\n",
    "# Get real graph conditioning\n",
    "cond_loader = create_dataloader(str(DATA_DIR), batch_size=1, shuffle=True,\n",
    "    use_vglc=True, normalize=True, load_graphs=True)\n",
    "conditionings, shapes = [], []\n",
    "for batch_data in cond_loader:\n",
    "    if isinstance(batch_data, (list, tuple)) and len(batch_data) == 2:\n",
    "        imgs, graphs = batch_data\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        for g in graphs:\n",
    "            try: conditionings.append(gen._encode_graph_conditioning(g))\n",
    "            except: pass\n",
    "        shapes.append(gen.encode_to_latent(imgs).shape)\n",
    "    if len(conditionings) >= NUM_SAMPLES: break\n",
    "\n",
    "# Generate\n",
    "gen.ema_diffusion.eval()\n",
    "generated = []\n",
    "with torch.no_grad():\n",
    "    for i in range(NUM_SAMPLES):\n",
    "        c = conditionings[i % len(conditionings)]\n",
    "        z = gen.ema_diffusion.sample(c, shape=shapes[0])\n",
    "        logits = gen.decode_from_latent(z)\n",
    "        tile_ids = logits.argmax(dim=1).squeeze(0).cpu().numpy()\n",
    "        generated.append(tile_ids)\n",
    "        print(f\"  Sample {i+1}: {tile_ids.shape}, {len(np.unique(tile_ids))} tile types\")\n",
    "\n",
    "# Visualize\n",
    "TILE_COLORS = {\n",
    "    0: '#1a1a2e', 1: '#e8d5b7', 2: '#4a4a4a', 3: '#8b7355',\n",
    "    10: '#90EE90', 11: '#FFD700', 12: '#FF6347', 13: '#9370DB',\n",
    "    14: '#DC143C', 15: '#FFA500', 20: '#FF4444', 21: '#00FF00',\n",
    "    22: '#FFD700', 23: '#8B0000', 30: '#FFFF00', 31: '#FF69B4',\n",
    "    32: '#00CED1', 33: '#87CEEB', 40: '#4169E1', 41: '#6495ED',\n",
    "    42: '#DEB887', 43: '#DA70D6',\n",
    "}\n",
    "\n",
    "def grid_to_rgb(grid):\n",
    "    h, w = grid.shape\n",
    "    rgb = np.full((h, w, 3), 0.5, dtype=np.float32)\n",
    "    for tid, hex_c in TILE_COLORS.items():\n",
    "        mask = grid == tid\n",
    "        if mask.any():\n",
    "            r, g, b = mcolors.hex2color(hex_c)\n",
    "            rgb[mask] = [r, g, b]\n",
    "    return rgb\n",
    "\n",
    "fig, axes = plt.subplots(1, NUM_SAMPLES, figsize=(5*NUM_SAMPLES, 8))\n",
    "if NUM_SAMPLES == 1: axes = [axes]\n",
    "for i, (ax, grid) in enumerate(zip(axes, generated)):\n",
    "    ax.imshow(grid_to_rgb(grid), interpolation='nearest', aspect='auto')\n",
    "    ax.set_title(f'Dungeon {i+1}\\n{grid.shape[0]}x{grid.shape[1]}')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('H-MOLQD Generated Dungeons (Graph-Conditioned)', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(OUTPUT_DIR / 'generated_samples.png'), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "for i, grid in enumerate(generated):\n",
    "    np.save(str(OUTPUT_DIR / f'dungeon_{i+1}.npy'), grid)\n",
    "print(f\"ðŸ’¾ Saved {NUM_SAMPLES} dungeons to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa6a985",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“¦ Summary & Download\n",
    "\n",
    "### Checkpoints Saved\n",
    "\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `checkpoints/vqvae_pretrained.pth` | VQ-VAE encoder/decoder (44 tiles, 512 codebook) |\n",
    "| `checkpoints/best_model.pth` | Best diffusion model (lowest val loss) |\n",
    "| `checkpoints/final_model.pth` | Final diffusion model (last epoch) |\n",
    "\n",
    "### Generated Outputs\n",
    "\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `outputs/generated_samples.png` | Visualization of generated dungeons |\n",
    "| `outputs/dungeon_*.npy` | Raw tile grids (NumPy arrays) |\n",
    "\n",
    "### Download Results (Kaggle)\n",
    "\n",
    "```python\n",
    "import shutil\n",
    "shutil.make_archive('/kaggle/working/hmolqd_results', 'zip', '/kaggle/working/hmolqd_outputs')\n",
    "```\n",
    "\n",
    "Then download `hmolqd_results.zip` from the **Output** tab.\n",
    "\n",
    "### Next Steps (Local)\n",
    "1. **WFC Refinement** (Block V): `python -m src.generation.wfc_refiner --input outputs/dungeon_1.npy`\n",
    "2. **Cognitive Validation** (Block VI): `python -m src.simulation.cognitive_validator --dungeon outputs/dungeon_1.npy`\n",
    "3. **Quality-Diversity Search**: Use MAP-Elites with trained models for diverse dungeon generation"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
