{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3409aa35",
   "metadata": {},
   "source": [
    "# üè∞ H-MOLQD: Hybrid Masked-diffusion Optimization with Logical Quality Diversity\n",
    "\n",
    "**GPU-Accelerated Training Notebook for Zelda Dungeon Generation**\n",
    "\n",
    "## Architecture Overview (6 Blocks)\n",
    "\n",
    "```\n",
    "VGLC Data (18 dungeons √ó .txt grids + .dot mission graphs)\n",
    "    ‚îÇ\n",
    "    ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Block I  ‚Äî VQ-VAE: 44 tiles ‚Üí 512 codebook ‚Üí latent_dim=64       ‚îÇ\n",
    "‚îÇ  Block II ‚Äî Dual-Stream Condition Encoder (GATv2Conv GNN + Local)  ‚îÇ\n",
    "‚îÇ  Block III‚Äî Latent Diffusion (U-Net, cosine schedule, DDIM)        ‚îÇ\n",
    "‚îÇ  Block IV ‚Äî LogicNet (differentiable solvability + key-lock check) ‚îÇ\n",
    "‚îÇ  Block V  ‚Äî WFC Refiner (inference-only post-processing)           ‚îÇ\n",
    "‚îÇ  Block VI ‚Äî Cognitive Validator (A* solver + CBS agent)            ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "## Training Pipeline\n",
    "- **Stage 1**: VQ-VAE pretraining (reconstructing dungeon grids)\n",
    "- **Stage 2**: Latent Diffusion with **real .dot graph conditioning** + LogicNet guidance\n",
    "\n",
    "## How to Use on Kaggle\n",
    "1. Upload your project repo as a **Kaggle Dataset**\n",
    "2. Enable **GPU accelerator** (T4/P100)\n",
    "3. Run all cells sequentially\n",
    "4. Download checkpoints from outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc616524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 1: Environment Setup\n",
    "# ============================================================================\n",
    "import os, sys, subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "IS_KAGGLE = os.path.exists('/kaggle/working')\n",
    "IS_COLAB = 'google.colab' in sys.modules\n",
    "ENV_NAME = \"Kaggle\" if IS_KAGGLE else (\"Colab\" if IS_COLAB else \"Local\")\n",
    "print(f\"üñ•Ô∏è  Environment: {ENV_NAME}\")\n",
    "\n",
    "# --- Find project root ---\n",
    "if IS_KAGGLE:\n",
    "    WORKING_DIR = Path('/kaggle/working')\n",
    "    candidates = [\n",
    "        Path('/kaggle/input/kltn'), Path('/kaggle/input/hmolqd'),\n",
    "        Path('/kaggle/input/kltn/KLTN'), Path('/kaggle/working/KLTN'),\n",
    "    ]\n",
    "elif IS_COLAB:\n",
    "    WORKING_DIR = Path('/content')\n",
    "    candidates = [Path('/content/KLTN'), Path('/content/drive/MyDrive/KLTN')]\n",
    "else:\n",
    "    WORKING_DIR = Path('.').resolve()\n",
    "    candidates = [Path('.').resolve(), Path('..').resolve()]\n",
    "\n",
    "PROJECT_ROOT = None\n",
    "for c in candidates:\n",
    "    if (c / 'src' / 'train_vqvae.py').exists():\n",
    "        PROJECT_ROOT = c\n",
    "        break\n",
    "\n",
    "if PROJECT_ROOT is None and not (not IS_KAGGLE and not IS_COLAB):\n",
    "    clone_target = WORKING_DIR / 'KLTN'\n",
    "    if not clone_target.exists():\n",
    "        subprocess.run(['git', 'clone', 'https://github.com/YOUR_USERNAME/KLTN.git',\n",
    "                        str(clone_target)], check=False)\n",
    "    if (clone_target / 'src' / 'train_vqvae.py').exists():\n",
    "        PROJECT_ROOT = clone_target\n",
    "\n",
    "if PROJECT_ROOT is None:\n",
    "    raise FileNotFoundError(\"‚ùå Could not find project root! Upload repo as Kaggle dataset.\")\n",
    "\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "CHECKPOINT_DIR = (WORKING_DIR if IS_KAGGLE else PROJECT_ROOT) / 'checkpoints'\n",
    "OUTPUT_DIR = (WORKING_DIR if IS_KAGGLE else PROJECT_ROOT) / 'output'\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / 'data' / 'The Legend of Zelda'\n",
    "if not DATA_DIR.exists():\n",
    "    DATA_DIR = PROJECT_ROOT / 'Data' / 'The Legend of Zelda'\n",
    "\n",
    "print(f\"‚úÖ Project root: {PROJECT_ROOT}\")\n",
    "print(f\"üìÅ Data dir: {DATA_DIR} (exists={DATA_DIR.exists()})\")\n",
    "print(f\"üíæ Checkpoints: {CHECKPOINT_DIR}\")\n",
    "\n",
    "# Install dependencies\n",
    "req = PROJECT_ROOT / 'requirements-hmolqd.txt'\n",
    "if req.exists():\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', '-r', str(req)], check=False)\n",
    "print(\"üéâ Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19356821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: Multi-Path Checkpoint Discovery System\n",
    "# ============================================================================\n",
    "# ‚ö†Ô∏è  IMPORTANT: This cell MUST be executed BEFORE training cells (VQ-VAE/Diffusion)\n",
    "# If you get \"unexpected keyword argument\" errors, restart kernel and run all cells in order\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List, Tuple, Dict\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class CheckpointInfo:\n",
    "    \"\"\"Metadata about a discovered checkpoint file.\"\"\"\n",
    "    path: Path\n",
    "    source_type: str  # 'working', 'input_dataset', 'notebook_output', 'direct_file'\n",
    "    source_location: str  # Human-readable source directory\n",
    "    file_size_mb: float\n",
    "    modified_time: float\n",
    "    \n",
    "    # Checkpoint content (extracted after validation)\n",
    "    epoch: Optional[int] = None\n",
    "    accuracy: Optional[float] = None\n",
    "    solvability: Optional[float] = None\n",
    "    loss: Optional[float] = None\n",
    "    \n",
    "    # Validation status\n",
    "    is_valid: bool = False\n",
    "    validation_msg: str = \"\"\n",
    "\n",
    "def find_checkpoint_locations() -> Dict[str, List[Path]]:\n",
    "    \"\"\"\n",
    "    Discover all potential checkpoint directories across Kaggle environment.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with keys: 'working', 'input_datasets', 'notebook_outputs', 'direct_files'\n",
    "    \"\"\"\n",
    "    locations = {\n",
    "        'working': [],\n",
    "        'input_datasets': [],\n",
    "        'notebook_outputs': [],\n",
    "        'direct_files': []\n",
    "    }\n",
    "    \n",
    "    # Priority 1: Working directory (writable, current run)\n",
    "    working_ckpt = Path('/kaggle/working/checkpoints')\n",
    "    if working_ckpt.exists():\n",
    "        locations['working'].append(working_ckpt)\n",
    "    \n",
    "    # Priority 2: Input datasets (read-only, uploaded runs)\n",
    "    input_base = Path('/kaggle/input')\n",
    "    if input_base.exists():\n",
    "        for dataset_dir in input_base.iterdir():\n",
    "            if not dataset_dir.is_dir() or dataset_dir.name == 'notebooks':\n",
    "                continue\n",
    "            \n",
    "            # Check common checkpoint subdirectories\n",
    "            for subdir_name in ['checkpoints', 'output', 'outputs']:\n",
    "                ckpt_subdir = dataset_dir / subdir_name\n",
    "                if ckpt_subdir.exists() and ckpt_subdir.is_dir():\n",
    "                    locations['input_datasets'].append(ckpt_subdir)\n",
    "            \n",
    "            # Also check dataset root for flat checkpoint structure\n",
    "            if any((dataset_dir / f).suffix == '.pth' for f in dataset_dir.iterdir() if f.is_file()):\n",
    "                locations['direct_files'].append(dataset_dir)\n",
    "    \n",
    "    # Priority 3: Notebook outputs (read-only, /kaggle/input/notebooks/user/notebook-name/)\n",
    "    notebook_base = Path('/kaggle/input/notebooks')\n",
    "    if notebook_base.exists():\n",
    "        for user_dir in notebook_base.iterdir():\n",
    "            if not user_dir.is_dir():\n",
    "                continue\n",
    "            for notebook_dir in user_dir.iterdir():\n",
    "                if not notebook_dir.is_dir():\n",
    "                    continue\n",
    "                \n",
    "                # Check for checkpoint subdirectories\n",
    "                for subdir_name in ['checkpoints', 'output', 'outputs']:\n",
    "                    ckpt_subdir = notebook_dir / subdir_name\n",
    "                    if ckpt_subdir.exists() and ckpt_subdir.is_dir():\n",
    "                        locations['notebook_outputs'].append(ckpt_subdir)\n",
    "                \n",
    "                # Check notebook root for flat structure\n",
    "                if any((notebook_dir / f).suffix == '.pth' for f in notebook_dir.iterdir() if f.is_file()):\n",
    "                    locations['direct_files'].append(notebook_dir)\n",
    "    \n",
    "    return locations\n",
    "\n",
    "def validate_and_load_checkpoint(\n",
    "    ckpt_path: Path,\n",
    "    required_keys: Optional[List[str]] = None,\n",
    "    source_type: str = '',\n",
    "    source_location: str = ''\n",
    ") -> CheckpointInfo:\n",
    "    \"\"\"\n",
    "    Validate checkpoint file and extract metadata.\n",
    "    \n",
    "    Args:\n",
    "        ckpt_path: Path to checkpoint file\n",
    "        required_keys: Keys that must be present in checkpoint\n",
    "        source_type: Type of source ('working', 'input_dataset', etc.)\n",
    "        source_location: Human-readable source directory\n",
    "        \n",
    "    Returns:\n",
    "        CheckpointInfo with all extracted metadata\n",
    "    \"\"\"\n",
    "    info = CheckpointInfo(\n",
    "        path=ckpt_path,\n",
    "        source_type=source_type,\n",
    "        source_location=source_location,\n",
    "        file_size_mb=ckpt_path.stat().st_size / (1024**2),\n",
    "        modified_time=ckpt_path.stat().st_mtime\n",
    "    )\n",
    "    \n",
    "    # Try to load and validate checkpoint\n",
    "    try:\n",
    "        ckpt = torch.load(ckpt_path, map_location='cpu', weights_only=False)\n",
    "        \n",
    "        # Validate required keys\n",
    "        if required_keys:\n",
    "            missing = [k for k in required_keys if k not in ckpt]\n",
    "            if missing:\n",
    "                info.is_valid = False\n",
    "                info.validation_msg = f\"Missing keys: {missing}\"\n",
    "                return info\n",
    "        \n",
    "        # Extract metadata\n",
    "        info.epoch = ckpt.get('epoch', None)\n",
    "        info.accuracy = ckpt.get('accuracy', None)\n",
    "        info.solvability = ckpt.get('val_solvability', ckpt.get('solvability', None))\n",
    "        info.loss = ckpt.get('loss', None)\n",
    "        \n",
    "        # Sanity checks\n",
    "        if info.epoch is not None and info.epoch < 0:\n",
    "            info.is_valid = False\n",
    "            info.validation_msg = \"Invalid epoch (< 0)\"\n",
    "            return info\n",
    "        \n",
    "        if info.accuracy is not None and not (0 <= info.accuracy <= 1):\n",
    "            info.is_valid = False\n",
    "            info.validation_msg = f\"Invalid accuracy ({info.accuracy})\"\n",
    "            return info\n",
    "        \n",
    "        info.is_valid = True\n",
    "        info.validation_msg = \"Valid\"\n",
    "        return info\n",
    "        \n",
    "    except Exception as e:\n",
    "        info.is_valid = False\n",
    "        info.validation_msg = f\"Load failed: {str(e)[:100]}\"\n",
    "        return info\n",
    "\n",
    "def find_best_checkpoint_across_sources(\n",
    "    checkpoint_filename: str = 'vqvae_pretrained.pth',\n",
    "    required_keys: Optional[List[str]] = None,\n",
    "    prefer_metric: Optional[str] = None  # 'accuracy', 'solvability', 'epoch'\n",
    ") -> Tuple[Optional[Path], Optional[CheckpointInfo], List[CheckpointInfo]]:\n",
    "    \"\"\"\n",
    "    Find the best checkpoint across ALL available sources in Kaggle.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_filename: Name of checkpoint to search for (e.g., 'vqvae_pretrained.pth')\n",
    "        required_keys: Keys that must be present in checkpoint\n",
    "        prefer_metric: Metric to prioritize when multiple checkpoints found\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (best_path, best_info, all_valid_checkpoints)\n",
    "        \n",
    "    Priority logic:\n",
    "        1. If prefer_metric specified: choose checkpoint with best metric value\n",
    "        2. Otherwise: prefer working > input_datasets > notebook_outputs > direct_files\n",
    "        3. Within same priority level: choose most recent (by modified time)\n",
    "    \"\"\"\n",
    "    locations = find_checkpoint_locations()\n",
    "    all_candidates: List[CheckpointInfo] = []\n",
    "    \n",
    "    # Search priority order\n",
    "    search_order = [\n",
    "        ('working', locations['working']),\n",
    "        ('input_datasets', locations['input_datasets']),\n",
    "        ('notebook_outputs', locations['notebook_outputs']),\n",
    "        ('direct_files', locations['direct_files'])\n",
    "    ]\n",
    "    \n",
    "    print(f\"üîç Searching for '{checkpoint_filename}' across all sources...\")\n",
    "    \n",
    "    for priority_idx, (source_type, dirs) in enumerate(search_order):\n",
    "        if not dirs:\n",
    "            continue\n",
    "        \n",
    "        for checkpoint_dir in dirs:\n",
    "            ckpt_path = checkpoint_dir / checkpoint_filename\n",
    "            if not ckpt_path.exists():\n",
    "                continue\n",
    "            \n",
    "            # Validate checkpoint\n",
    "            info = validate_and_load_checkpoint(\n",
    "                ckpt_path,\n",
    "                required_keys=required_keys,\n",
    "                source_type=source_type,\n",
    "                source_location=str(checkpoint_dir)\n",
    "            )\n",
    "            \n",
    "            # Display found checkpoint\n",
    "            status_icon = \"‚úÖ\" if info.is_valid else \"‚ùå\"\n",
    "            print(f\"   {status_icon} [{source_type:15s}] {ckpt_path}\")\n",
    "            \n",
    "            if info.is_valid:\n",
    "                metrics_parts = [f\"{info.file_size_mb:.1f}MB\"]\n",
    "                if info.epoch is not None:\n",
    "                    metrics_parts.append(f\"epoch={info.epoch}\")\n",
    "                if info.accuracy is not None:\n",
    "                    metrics_parts.append(f\"acc={info.accuracy:.3f}\")\n",
    "                if info.solvability is not None:\n",
    "                    metrics_parts.append(f\"solv={info.solvability:.3f}\")\n",
    "                print(f\"      {', '.join(metrics_parts)} - {info.validation_msg}\")\n",
    "                \n",
    "                all_candidates.append((info, priority_idx))\n",
    "            else:\n",
    "                print(f\"      Invalid: {info.validation_msg}\")\n",
    "    \n",
    "    # No valid checkpoints found\n",
    "    if not all_candidates:\n",
    "        print(\"   ‚ùå No valid checkpoints found\")\n",
    "        return None, None, []\n",
    "    \n",
    "    # Select best checkpoint\n",
    "    if prefer_metric:\n",
    "        # Choose by best metric value\n",
    "        if prefer_metric == 'accuracy':\n",
    "            best_info, _ = max(all_candidates, key=lambda x: (x[0].accuracy or 0, -x[1]))\n",
    "            metric_display = f\"accuracy={best_info.accuracy:.3f}\"\n",
    "        elif prefer_metric == 'solvability':\n",
    "            best_info, _ = max(all_candidates, key=lambda x: (x[0].solvability or 0, -x[1]))\n",
    "            metric_display = f\"solvability={best_info.solvability:.3f}\"\n",
    "        elif prefer_metric == 'epoch':\n",
    "            best_info, _ = max(all_candidates, key=lambda x: (x[0].epoch or 0, -x[1]))\n",
    "            metric_display = f\"epoch={best_info.epoch}\"\n",
    "        else:\n",
    "            # Unknown metric - fall back to priority\n",
    "            best_info, _ = min(all_candidates, key=lambda x: (x[1], -x[0].modified_time))\n",
    "            metric_display = \"priority\"\n",
    "        \n",
    "        print(f\"\\nüéØ Selected checkpoint by best {prefer_metric}: {metric_display}\")\n",
    "    else:\n",
    "        # Choose by priority (working > input > notebook > direct)\n",
    "        best_info, _ = min(all_candidates, key=lambda x: (x[1], -x[0].modified_time))\n",
    "        print(f\"\\nüéØ Selected checkpoint by priority: {best_info.source_type}\")\n",
    "    \n",
    "    print(f\"   üìÇ {best_info.path}\")\n",
    "    print(f\"   üìä Epoch {best_info.epoch or 'N/A'}\", end='')\n",
    "    if best_info.accuracy:\n",
    "        print(f\", accuracy={best_info.accuracy:.3f}\", end='')\n",
    "    if best_info.solvability:\n",
    "        print(f\", solvability={best_info.solvability:.3f}\", end='')\n",
    "    print()\n",
    "    \n",
    "    all_valid_infos = [info for info, _ in all_candidates]\n",
    "    return best_info.path, best_info, all_valid_infos\n",
    "\n",
    "def copy_checkpoint_to_working(\n",
    "    source_path: Path,\n",
    "    target_filename: str,\n",
    "    working_dir: Path = Path('/kaggle/working/checkpoints')\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Copy checkpoint from read-only input to writable working directory.\n",
    "    \n",
    "    Args:\n",
    "        source_path: Source checkpoint path (may be read-only)\n",
    "        target_filename: Target filename in working directory\n",
    "        working_dir: Working directory path (default: /kaggle/working/checkpoints)\n",
    "        \n",
    "    Returns:\n",
    "        Path to copied checkpoint in working directory\n",
    "    \"\"\"\n",
    "    import shutil\n",
    "    \n",
    "    # Create working directory if needed\n",
    "    working_dir.mkdir(parents=True, exist_ok=True)\n",
    "    target_path = working_dir / target_filename\n",
    "    \n",
    "    # If source is already in working directory, no copy needed\n",
    "    if str(source_path).startswith(str(working_dir)):\n",
    "        return source_path\n",
    "    \n",
    "    # Copy file\n",
    "    shutil.copy2(source_path, target_path)\n",
    "    print(f\"   üìã Copied to working: {target_path}\")\n",
    "    \n",
    "    return target_path\n",
    "\n",
    "def discover_and_validate_all_checkpoints(show_invalid: bool = False) -> Dict[str, List[CheckpointInfo]]:\n",
    "    \"\"\"\n",
    "    Comprehensive scan of all checkpoint files across all sources.\n",
    "    Useful for debugging and understanding what's available.\n",
    "    \n",
    "    Args:\n",
    "        show_invalid: Whether to print invalid checkpoints\n",
    "        \n",
    "    Returns:\n",
    "        Dict mapping checkpoint filename to list of CheckpointInfo objects\n",
    "    \"\"\"\n",
    "    locations = find_checkpoint_locations()\n",
    "    all_checkpoints: Dict[str, List[CheckpointInfo]] = {}\n",
    "    \n",
    "    print(\"üîé Comprehensive checkpoint scan:\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    search_order = [\n",
    "        ('working', locations['working']),\n",
    "        ('input_datasets', locations['input_datasets']),\n",
    "        ('notebook_outputs', locations['notebook_outputs']),\n",
    "        ('direct_files', locations['direct_files'])\n",
    "    ]\n",
    "    \n",
    "    for source_type, dirs in search_order:\n",
    "        if not dirs:\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nüìÅ {source_type.upper().replace('_', ' ')}:\")\n",
    "        for checkpoint_dir in dirs:\n",
    "            print(f\"   {checkpoint_dir}\")\n",
    "            \n",
    "            # Find all .pth files\n",
    "            pth_files = list(checkpoint_dir.glob('*.pth'))\n",
    "            if not pth_files:\n",
    "                print(\"      (no .pth files found)\")\n",
    "                continue\n",
    "            \n",
    "            for pth_file in pth_files:\n",
    "                info = validate_and_load_checkpoint(\n",
    "                    pth_file,\n",
    "                    required_keys=None,  # No requirements for discovery\n",
    "                    source_type=source_type,\n",
    "                    source_location=str(checkpoint_dir)\n",
    "                )\n",
    "                \n",
    "                if info.is_valid or show_invalid:\n",
    "                    status = \"‚úÖ\" if info.is_valid else \"‚ùå\"\n",
    "                    metrics = []\n",
    "                    if info.epoch is not None:\n",
    "                        metrics.append(f\"epoch={info.epoch}\")\n",
    "                    if info.accuracy is not None:\n",
    "                        metrics.append(f\"acc={info.accuracy:.3f}\")\n",
    "                    if info.solvability is not None:\n",
    "                        metrics.append(f\"solv={info.solvability:.3f}\")\n",
    "                    \n",
    "                    metric_str = f\" ({', '.join(metrics)})\" if metrics else \"\"\n",
    "                    print(f\"      {status} {pth_file.name}{metric_str}\")\n",
    "                    if not info.is_valid:\n",
    "                        print(f\"         ‚ö†Ô∏è  {info.validation_msg}\")\n",
    "                \n",
    "                # Add to results\n",
    "                filename = pth_file.name\n",
    "                if filename not in all_checkpoints:\n",
    "                    all_checkpoints[filename] = []\n",
    "                all_checkpoints[filename].append(info)\n",
    "    \n",
    "    print(f\"\\nüìä Summary: Found {sum(len(v) for v in all_checkpoints.values())} checkpoint files\")\n",
    "    return all_checkpoints\n",
    "\n",
    "print(\"‚úÖ Multi-path checkpoint discovery system loaded\")\n",
    "print(\"   Functions available:\")\n",
    "print(\"   - find_best_checkpoint_across_sources()\")\n",
    "print(\"   - copy_checkpoint_to_working()\")\n",
    "print(\"   - discover_and_validate_all_checkpoints()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffd685a",
   "metadata": {},
   "source": [
    "---\n",
    "## üîÑ Multi-Path Checkpoint Discovery System\n",
    "\n",
    "**Purpose**: Automatically find and resume from checkpoints across multiple Kaggle runs.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "The system searches **ALL** possible checkpoint locations:\n",
    "1. `/kaggle/working/checkpoints/` - Current run (writable)\n",
    "2. `/kaggle/input/*/checkpoints/` - Previous runs uploaded as datasets (read-only)\n",
    "3. `/kaggle/input/notebooks/*/*/checkpoints/` - Auto-versioned notebook outputs\n",
    "\n",
    "### User Workflow (Zero Configuration!)\n",
    "\n",
    "**First Run (0-9 hours)**:\n",
    "```\n",
    "Start notebook ‚Üí Train ‚Üí Download outputs ‚Üí Upload as Kaggle dataset\n",
    "```\n",
    "\n",
    "**Resume Run (9+ hours)**:\n",
    "```\n",
    "Add dataset to inputs ‚Üí Run notebook ‚Üí Automatically resumes from best checkpoint!\n",
    "```\n",
    "\n",
    "### Key Features\n",
    "\n",
    "‚úÖ **Automatic Discovery** - Finds checkpoints from ANY previous run  \n",
    "‚úÖ **Intelligent Selection** - Chooses best checkpoint by accuracy or epoch  \n",
    "‚úÖ **Validation** - Checks checkpoint integrity before loading  \n",
    "‚úÖ **Auto-Copy** - Handles read-only ‚Üí writable directory  \n",
    "‚úÖ **Detailed Logging** - Shows exactly what was found and why  \n",
    "‚úÖ **Error Recovery** - Falls back to fresh training if no valid checkpoint  \n",
    "\n",
    "### Example Output\n",
    "\n",
    "```\n",
    "üîç Searching for checkpoints across all sources...\n",
    "   ‚úÖ [working        ] working/checkpoints/vqvae_pretrained.pth\n",
    "      23.4MB, epoch=50, acc=0.875 - Valid\n",
    "   ‚úÖ [input_datasets ] input/hmolqd-run1/checkpoints/vqvae_pretrained.pth\n",
    "      23.1MB, epoch=45, acc=0.860 - Valid\n",
    "\n",
    "üéØ Selected checkpoint by best accuracy:\n",
    "   üìÇ working/checkpoints/vqvae_pretrained.pth\n",
    "   üìä Epoch 50, accuracy=0.875\n",
    "```\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "**Checkpoint not found?**\n",
    "```python\n",
    "discover_and_validate_all_checkpoints(show_invalid=True)\n",
    "```\n",
    "\n",
    "### Documentation\n",
    "\n",
    "- üìñ **Full Guide**: [`docs/KAGGLE_CHECKPOINT_RESUME_GUIDE.md`](../docs/KAGGLE_CHECKPOINT_RESUME_GUIDE.md)\n",
    "- üìÑ **Quick Ref**: [`docs/KAGGLE_CHECKPOINT_QUICK_REF.md`](../docs/KAGGLE_CHECKPOINT_QUICK_REF.md)\n",
    "- üìä **Visual Guide**: [`docs/KAGGLE_CHECKPOINT_VISUAL_GUIDE.md`](../docs/KAGGLE_CHECKPOINT_VISUAL_GUIDE.md)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f07e79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: Import Verification & GPU Check\n",
    "# ============================================================================\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(levelname)s | %(message)s', datefmt='%H:%M:%S')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} ({torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB)\")\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "# Import project modules\n",
    "from src.core.vqvae import SemanticVQVAE, create_vqvae, VQVAETrainer\n",
    "from src.core.latent_diffusion import create_latent_diffusion\n",
    "from src.core.condition_encoder import create_condition_encoder\n",
    "from src.core.logic_net import LogicNet\n",
    "from src.data.zelda_loader import create_dataloader, graph_collate_fn\n",
    "from src.train_vqvae import grids_to_onehot\n",
    "from src.train_diffusion import DiffusionTrainingConfig, DiffusionTrainer\n",
    "print(\"‚úÖ All modules imported\")\n",
    "\n",
    "# Quick data test\n",
    "loader_test = create_dataloader(str(DATA_DIR), batch_size=2, use_vglc=True, normalize=True, load_graphs=True)\n",
    "print(f\"\\nüìä Dataset: {len(loader_test.dataset)} dungeons\")\n",
    "for batch in loader_test:\n",
    "    if isinstance(batch, (list, tuple)):\n",
    "        imgs, graphs = batch\n",
    "        g = graphs[0]\n",
    "        print(f\"   Image batch: {imgs.shape}\")\n",
    "        print(f\"   Graph[0]: {g['num_nodes']} nodes, {g['num_edges']} edges, features={g['node_features'].shape}\")\n",
    "    break\n",
    "print(\"‚úÖ Data loading verified with real .dot graphs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aed0790",
   "metadata": {},
   "source": [
    "---\n",
    "## üîµ Stage 1: VQ-VAE Pretraining (Block I)\n",
    "\n",
    "Train the Semantic VQ-VAE to reconstruct dungeon grids.\n",
    "- **Input**: 44-class one-hot tiles `[B, 44, H, W]`\n",
    "- **Codebook**: 512 embeddings, latent_dim=64\n",
    "- **Goal**: ‚â•85% reconstruction accuracy before Stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f582648a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: Stage 1 ‚Äî VQ-VAE Pretraining (PRODUCTION VERSION)\n",
    "# ============================================================================\n",
    "import json, time\n",
    "from pathlib import Path\n",
    "\n",
    "VQVAE_EPOCHS = 300           # Increase to 200-300 for production\n",
    "VQVAE_BATCH_SIZE = 2\n",
    "VQVAE_LR = 3e-4\n",
    "VQVAE_TARGET_ACCURACY = 0.85\n",
    "VQVAE_SAVE_PATH = CHECKPOINT_DIR / 'vqvae_pretrained.pth'\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üîµ STAGE 1: VQ-VAE PRETRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def validate_vqvae_checkpoint(ckpt_path):\n",
    "    \"\"\"Validate VQ-VAE checkpoint integrity.\"\"\"\n",
    "    required_keys = ['epoch', 'model_state_dict', 'optimizer_state_dict', 'accuracy']\n",
    "    try:\n",
    "        ckpt = torch.load(ckpt_path, map_location='cpu', weights_only=False)\n",
    "        missing = [k for k in required_keys if k not in ckpt]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing keys: {missing}\")\n",
    "        # Sanity checks\n",
    "        if ckpt['epoch'] < 0:\n",
    "            raise ValueError(\"Invalid epoch\")\n",
    "        if not (0 <= ckpt['accuracy'] <= 1):\n",
    "            raise ValueError(\"Invalid accuracy\")\n",
    "        return ckpt\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Invalid checkpoint: {e}\")\n",
    "        return None\n",
    "\n",
    "# Smart resume logic with MULTI-PATH checkpoint discovery\n",
    "SKIP_VQVAE = False\n",
    "resume_epoch = 0\n",
    "vqvae_history = []\n",
    "resume_checkpoint = None\n",
    "\n",
    "# Search for VQ-VAE checkpoints across ALL available sources\n",
    "best_ckpt_path, best_ckpt_info, all_ckpts = find_best_checkpoint_across_sources(\n",
    "    checkpoint_filename='vqvae_pretrained.pth',\n",
    "    required_keys=['epoch', 'model_state_dict', 'optimizer_state_dict', 'accuracy'],\n",
    "    prefer_metric='accuracy'  # Choose checkpoint with highest accuracy\n",
    ")\n",
    "\n",
    "if best_ckpt_path is not None:\n",
    "    # Load the best checkpoint found\n",
    "    ckpt = torch.load(best_ckpt_path, map_location='cpu', weights_only=False)\n",
    "    existing_acc = ckpt['accuracy']\n",
    "    resume_epoch = ckpt['epoch'] + 1\n",
    "    resume_checkpoint = ckpt\n",
    "    vqvae_history = ckpt.get('history', [])\n",
    "    \n",
    "    print(f\"‚ö° Loaded VQ-VAE checkpoint from: {best_ckpt_info.source_location}\")\n",
    "    print(f\"   Epoch {resume_epoch}, accuracy={existing_acc:.3f}\")\n",
    "    \n",
    "    # Copy to working directory if from input dataset (allows overwriting with better checkpoints)\n",
    "    if IS_KAGGLE and best_ckpt_info.source_type != 'working':\n",
    "        VQVAE_SAVE_PATH = copy_checkpoint_to_working(best_ckpt_path, 'vqvae_pretrained.pth')\n",
    "        print(f\"   üìã Copied to working directory for incremental saves\")\n",
    "    else:\n",
    "        VQVAE_SAVE_PATH = best_ckpt_path\n",
    "    \n",
    "    if existing_acc >= VQVAE_TARGET_ACCURACY:\n",
    "        print(f\"   ‚úÖ Accuracy ‚â• {VQVAE_TARGET_ACCURACY} ‚Äî skipping training\")\n",
    "        SKIP_VQVAE = True\n",
    "    else:\n",
    "        print(f\"   üîÑ Resuming from epoch {resume_epoch}\")\n",
    "        if vqvae_history:\n",
    "            print(f\"   üìä Loaded {len(vqvae_history)} epochs of history\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  No existing VQ-VAE checkpoint found - starting fresh training\")\n",
    "    # Ensure VQVAE_SAVE_PATH points to working directory\n",
    "    if IS_KAGGLE:\n",
    "        VQVAE_SAVE_PATH = Path('/kaggle/working/checkpoints') / 'vqvae_pretrained.pth'\n",
    "    else:\n",
    "        VQVAE_SAVE_PATH = CHECKPOINT_DIR / 'vqvae_pretrained.pth'\n",
    "\n",
    "if not SKIP_VQVAE:\n",
    "    # Create model and trainer with CORRECT parameter names\n",
    "    vqvae_model = create_vqvae(\n",
    "        num_classes=44, \n",
    "        latent_dim=64, \n",
    "        codebook_size=512  # ‚úÖ FIXED: was 'num_embeddings'\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    # ‚úÖ FIXED: use 'lr' instead of 'learning_rate', removed 'device' parameter\n",
    "    trainer = VQVAETrainer(vqvae_model, lr=VQVAE_LR)\n",
    "    \n",
    "    # Create dataloader\n",
    "    dataloader = create_dataloader(\n",
    "        str(DATA_DIR), \n",
    "        batch_size=VQVAE_BATCH_SIZE,\n",
    "        shuffle=True, \n",
    "        use_vglc=True, \n",
    "        normalize=True, \n",
    "        load_graphs=False\n",
    "    )\n",
    "    print(f\"üìä Training: {len(dataloader.dataset)} dungeons, {len(dataloader)} batches/epoch\")\n",
    "    \n",
    "    # Resume model and optimizer states AFTER creating model/trainer\n",
    "    if resume_checkpoint is not None:\n",
    "        try:\n",
    "            vqvae_model.load_state_dict(resume_checkpoint['model_state_dict'])\n",
    "            trainer.optimizer.load_state_dict(resume_checkpoint['optimizer_state_dict'])\n",
    "            print(f\"   üîÑ Resumed model and optimizer state\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Could not resume state: {e}\")\n",
    "            print(f\"   Starting fresh from epoch 0\")\n",
    "            resume_epoch = 0\n",
    "            vqvae_history = []\n",
    "    \n",
    "    # Training tracking\n",
    "    best_loss = float('inf')\n",
    "    if vqvae_history:\n",
    "        best_loss = min(h['loss'] for h in vqvae_history)\n",
    "        print(f\"   üìà Previous best loss: {best_loss:.4f}\")\n",
    "    \n",
    "    history = vqvae_history\n",
    "    t0 = time.time()\n",
    "    \n",
    "    print(f\"\\n{'Epoch':>6} | {'Loss':>8} | {'Recon':>8} | {'VQ':>6} | {'Acc':>6} | {'Time':>6}\")\n",
    "    print(\"‚îÄ\" * 60)\n",
    "\n",
    "    for epoch in range(resume_epoch, VQVAE_EPOCHS):\n",
    "        # Training loop\n",
    "        vqvae_model.train()\n",
    "        metrics_sum = {'loss': 0, 'recon_loss': 0, 'vq_loss': 0, 'perplexity': 0}\n",
    "        nb = 0\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            if isinstance(batch, (list, tuple)):\n",
    "                batch = batch[0]\n",
    "            batch = batch.to(DEVICE)\n",
    "            x_onehot = grids_to_onehot(batch, num_classes=44)\n",
    "            _, m = trainer.train_step(x_onehot)\n",
    "            for k in metrics_sum:\n",
    "                metrics_sum[k] += m.get(k, 0.0)\n",
    "            nb += 1\n",
    "        \n",
    "        # Average metrics\n",
    "        for k in metrics_sum:\n",
    "            metrics_sum[k] /= max(nb, 1)\n",
    "\n",
    "        # Evaluation\n",
    "        vqvae_model.eval()\n",
    "        acc_sum, acc_n = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                if isinstance(batch, (list, tuple)):\n",
    "                    batch = batch[0]\n",
    "                batch = batch.to(DEVICE)\n",
    "                x_onehot = grids_to_onehot(batch, num_classes=44)\n",
    "                recon, _ = vqvae_model(x_onehot)\n",
    "                pred_tiles = recon.argmax(dim=1)\n",
    "                orig_tiles = x_onehot.argmax(dim=1)\n",
    "                acc_sum += (pred_tiles == orig_tiles).float().mean().item()\n",
    "                acc_n += 1\n",
    "        eval_acc = acc_sum / max(acc_n, 1)\n",
    "\n",
    "        # Record history\n",
    "        history.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'loss': metrics_sum['loss'],\n",
    "            'recon_loss': metrics_sum['recon_loss'],\n",
    "            'vq_loss': metrics_sum['vq_loss'],\n",
    "            'perplexity': metrics_sum['perplexity'],\n",
    "            'accuracy': eval_acc\n",
    "        })\n",
    "        \n",
    "        # Save checkpoint (atomic write to prevent corruption)\n",
    "        if metrics_sum['loss'] < best_loss or eval_acc >= VQVAE_TARGET_ACCURACY:\n",
    "            checkpoint_state = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': vqvae_model.state_dict(),\n",
    "                'optimizer_state_dict': trainer.optimizer.state_dict(),\n",
    "                'loss': metrics_sum['loss'],\n",
    "                'accuracy': eval_acc,\n",
    "                'perplexity': metrics_sum['perplexity'],\n",
    "                'history': history\n",
    "            }\n",
    "            \n",
    "            # Atomic save (temp file ‚Üí rename) to prevent corruption during write\n",
    "            temp_path = VQVAE_SAVE_PATH.parent / f\".{VQVAE_SAVE_PATH.name}.tmp\"\n",
    "            torch.save(checkpoint_state, temp_path)\n",
    "            if VQVAE_SAVE_PATH.exists():\n",
    "                VQVAE_SAVE_PATH.unlink()\n",
    "            temp_path.rename(VQVAE_SAVE_PATH)\n",
    "            \n",
    "            best_loss = metrics_sum['loss']\n",
    "        \n",
    "        # Progress logging\n",
    "        if (epoch + 1) % 5 == 0 or epoch == resume_epoch or epoch == 0:\n",
    "            elapsed = time.time() - t0\n",
    "            print(f\"  {epoch+1:4d}   | {metrics_sum['loss']:.4f}  | \"\n",
    "                  f\"{metrics_sum['recon_loss']:.4f}  | {metrics_sum['vq_loss']:.4f} | \"\n",
    "                  f\"{eval_acc:.3f} | {elapsed/60:.1f}m\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if eval_acc >= VQVAE_TARGET_ACCURACY:\n",
    "            print(f\"\\nüéØ Target accuracy {VQVAE_TARGET_ACCURACY:.2f} reached!\")\n",
    "            break\n",
    "\n",
    "    print(f\"\\n‚úÖ VQ-VAE complete! Best loss={best_loss:.4f}, Final acc={eval_acc:.3f}\")\n",
    "    print(f\"   Checkpoint: {VQVAE_SAVE_PATH}\")\n",
    "    \n",
    "    # Save training history separately (for analysis)\n",
    "    history_path = CHECKPOINT_DIR / 'vqvae_history.json'\n",
    "    with open(history_path, 'w') as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "    \n",
    "    # Plot training curves\n",
    "    if len(history) > 1:\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "        ep_x = [h['epoch'] for h in history]\n",
    "        axes[0].plot(ep_x, [h['loss'] for h in history], 'b-')\n",
    "        axes[0].set_title('Total Loss'); axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        axes[1].plot(ep_x, [h['recon_loss'] for h in history], 'r-', label='Recon')\n",
    "        axes[1].plot(ep_x, [h['vq_loss'] for h in history], 'g-', label='VQ')\n",
    "        axes[1].set_title('Loss Components'); axes[1].legend(); axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        axes[2].plot(ep_x, [h['accuracy'] for h in history], 'm-')\n",
    "        axes[2].set_title('Accuracy'); axes[2].set_ylim(0, 1); axes[2].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.suptitle('Stage 1: VQ-VAE Training', fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(str(OUTPUT_DIR / 'vqvae_curves.png'), dpi=150)\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  Skipping VQ-VAE training - target accuracy achieved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010b1ec6",
   "metadata": {},
   "source": [
    "---\n",
    "## üü¢ Stage 2: Latent Diffusion with Real Graph Conditioning\n",
    "\n",
    "Full pipeline training using **real .dot mission graphs** from VGLC:\n",
    "- **U-Net** denoiser in VQ-VAE latent space\n",
    "- **GATv2Conv GNN** encodes real dungeon graph topology (nodes=rooms, edges=doors)\n",
    "- **LogicNet** gradient guidance: differentiable solvability + key-lock checking\n",
    "- **EMA** model weights for stable sampling\n",
    "\n",
    "| Phase | Epochs | Loss Components |\n",
    "|-------|--------|-----------------|\n",
    "| Warmup | 1‚Äì5 | Diffusion only (no logic) |\n",
    "| Full | 6+ | Diffusion + Œ±√óLogicNet |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b867f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 4: Stage 2 ‚Äî Diffusion Training with Real Graph Conditioning (PRODUCTION VERSION)\n",
    "# ============================================================================\n",
    "import time\n",
    "\n",
    "# Helper functions for compact checkpointing (Kaggle disk space optimization)\n",
    "def _save_minimal_checkpoint(ckpt_path, trainer, metrics_dict):\n",
    "    \"\"\"Save checkpoint without optimizer/scheduler states to save disk space.\"\"\"\n",
    "    state = {\n",
    "        'epoch': trainer.epoch,\n",
    "        'model_state_dict': trainer.ema_diffusion.state_dict(),\n",
    "        'ema_state_dict': trainer.ema_diffusion.state_dict(),\n",
    "        'condition_encoder_state_dict': trainer.condition_encoder.state_dict(),\n",
    "        'logic_net_state_dict': trainer.logic_net.state_dict()\n",
    "    }\n",
    "    if metrics_dict:\n",
    "        state.update(metrics_dict)\n",
    "    \n",
    "    torch.save(state, ckpt_path)\n",
    "    print(f\"üíæ Saved compact checkpoint: {ckpt_path.name} ({ckpt_path.stat().st_size / 1024**2:.1f}MB)\")\n",
    "\n",
    "def _prune_checkpoints(checkpoint_dir, keep=3, pattern='checkpoint_*.pth'):\n",
    "    \"\"\"Keep only the most recent N checkpoints to save disk space.\"\"\"\n",
    "    checkpoints = sorted(checkpoint_dir.glob(pattern), key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "    for old_ckpt in checkpoints[keep:]:\n",
    "        size_mb = old_ckpt.stat().st_size / 1024**2\n",
    "        old_ckpt.unlink()\n",
    "        print(f\"üóëÔ∏è  Pruned: {old_ckpt.name} ({size_mb:.1f} MB freed)\")\n",
    "\n",
    "def verify_checkpoint_integrity(ckpt_path, required_keys):\n",
    "    \"\"\"Validate checkpoint before loading.\"\"\"\n",
    "    try:\n",
    "        ckpt = torch.load(ckpt_path, map_location='cpu', weights_only=False)\n",
    "        missing = [k for k in required_keys if k not in ckpt]\n",
    "        if missing:\n",
    "            return False, f\"Missing keys: {missing}\"\n",
    "        # Check epoch sanity\n",
    "        if 'epoch' in ckpt and ckpt['epoch'] < 0:\n",
    "            return False, \"Invalid epoch value\"\n",
    "        return True, \"Valid\"\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "DIFFUSION_EPOCHS = 500       # 100+ recommended; 500 for production\n",
    "DIFFUSION_BATCH_SIZE = 2\n",
    "DIFFUSION_LR = 1e-4\n",
    "ALPHA_LOGIC = 0.1\n",
    "WARMUP_EPOCHS = 5\n",
    "\n",
    "# Checkpointing policy: compact checkpoints for Kaggle (reduce disk usage)\n",
    "if IS_KAGGLE:\n",
    "    SAVE_EVERY = 50  # less frequent full saves\n",
    "    MAX_KEEP_CHECKPOINTS = 3\n",
    "    COMPACT_CHECKPOINTS = True\n",
    "else:\n",
    "    SAVE_EVERY = 10\n",
    "    MAX_KEEP_CHECKPOINTS = 10\n",
    "    COMPACT_CHECKPOINTS = False\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üü¢ STAGE 2: LATENT DIFFUSION TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if not VQVAE_SAVE_PATH.exists():\n",
    "    raise FileNotFoundError(f\"‚ùå VQ-VAE checkpoint not found. Run Stage 1 first!\")\n",
    "\n",
    "config = DiffusionTrainingConfig(\n",
    "    data_dir=str(DATA_DIR), batch_size=DIFFUSION_BATCH_SIZE, use_vglc=True,\n",
    "    vqvae_checkpoint=str(VQVAE_SAVE_PATH), latent_dim=64, model_channels=128,\n",
    "    context_dim=256, num_timesteps=1000, schedule_type='cosine',\n",
    "    num_logic_iterations=30, guidance_scale=1.0, epochs=DIFFUSION_EPOCHS,\n",
    "    learning_rate=DIFFUSION_LR, alpha_visual=1.0, alpha_logic=ALPHA_LOGIC,\n",
    "    warmup_epochs=WARMUP_EPOCHS, checkpoint_dir=str(CHECKPOINT_DIR),\n",
    "    save_every=SAVE_EVERY, device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    ")\n",
    "\n",
    "# Data loaders with REAL graph data from .dot files\n",
    "train_loader = create_dataloader(config.data_dir, batch_size=config.batch_size,\n",
    "    shuffle=True, use_vglc=True, normalize=True, load_graphs=True)\n",
    "val_loader = create_dataloader(config.data_dir, batch_size=config.batch_size,\n",
    "    shuffle=False, use_vglc=True, normalize=True, load_graphs=True)\n",
    "print(f\"üìä {len(train_loader.dataset)} dungeons with real .dot graphs, {len(train_loader)} batches/epoch\")\n",
    "\n",
    "# Create trainer\n",
    "diff_trainer = DiffusionTrainer(config)\n",
    "print(f\"üèóÔ∏è  All models on {config.device}, VQ-VAE frozen ‚úÖ\")\n",
    "\n",
    "# Smart resume logic: find the latest valid checkpoint across ALL sources\n",
    "def find_latest_checkpoint_multi_source(\n",
    "    checkpoint_patterns: List[str] = ['final_model.pth', 'best_model.pth', 'checkpoint_*.pth'],\n",
    "    required_keys: Optional[List[str]] = None\n",
    ") -> Tuple[Optional[Path], Optional[CheckpointInfo]]:\n",
    "    \"\"\"\n",
    "    Find the most recent valid checkpoint across all Kaggle sources.\n",
    "    Supports glob patterns like 'checkpoint_*.pth' to find numbered checkpoints.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (best_checkpoint_path, checkpoint_info)\n",
    "    \"\"\"\n",
    "    all_candidates: List[CheckpointInfo] = []\n",
    "    locations = find_checkpoint_locations()\n",
    "    \n",
    "    print(\"üîç Searching for diffusion checkpoints...\")\n",
    "    \n",
    "    search_order = [\n",
    "        ('working', locations['working']),\n",
    "        ('input_datasets', locations['input_datasets']),\n",
    "        ('notebook_outputs', locations['notebook_outputs']),\n",
    "        ('direct_files', locations['direct_files'])\n",
    "    ]\n",
    "    \n",
    "    for source_type, dirs in search_order:\n",
    "        for checkpoint_dir in dirs:\n",
    "            for pattern in checkpoint_patterns:\n",
    "                # Handle glob patterns\n",
    "                if '*' in pattern:\n",
    "                    matches = sorted(checkpoint_dir.glob(pattern), \n",
    "                                   key=lambda p: int(p.stem.split('_')[-1]) if p.stem.split('_')[-1].isdigit() else 0,\n",
    "                                   reverse=True)\n",
    "                else:\n",
    "                    matches = [checkpoint_dir / pattern] if (checkpoint_dir / pattern).exists() else []\n",
    "                \n",
    "                for ckpt_path in matches:\n",
    "                    if not ckpt_path.exists():\n",
    "                        continue\n",
    "                    \n",
    "                    # ‚úÖ FIXED: Use validate_and_load_checkpoint from Cell 3\n",
    "                    info = validate_and_load_checkpoint(\n",
    "                        ckpt_path=ckpt_path,\n",
    "                        required_keys=required_keys,\n",
    "                        source_type=source_type,\n",
    "                        source_location=str(checkpoint_dir.relative_to('/kaggle') if IS_KAGGLE else checkpoint_dir)\n",
    "                    )\n",
    "                    \n",
    "                    if info.is_valid:\n",
    "                        all_candidates.append(info)\n",
    "                        status = \"‚úÖ\"\n",
    "                        epoch_str = f\"epoch={info.epoch}\" if info.epoch is not None else \"epoch=?\"\n",
    "                        solv_str = f\", solv={info.solvability:.3f}\" if info.solvability else \"\"\n",
    "                        print(f\"   {status} [{source_type:15}] {ckpt_path.name} - {epoch_str}{solv_str}\")\n",
    "    \n",
    "    if not all_candidates:\n",
    "        print(\"‚ÑπÔ∏è  No valid diffusion checkpoints found\")\n",
    "        return None, None\n",
    "    \n",
    "    # Sort by epoch (most recent first), then by source priority\n",
    "    priority_map = {'working': 0, 'input_datasets': 1, 'notebook_outputs': 2, 'direct_files': 3}\n",
    "    all_candidates.sort(\n",
    "        key=lambda c: (-(c.epoch if c.epoch is not None else -1), priority_map.get(c.source_type, 99))\n",
    "    )\n",
    "    \n",
    "    best = all_candidates[0]\n",
    "    print(f\"\\nüéØ Selected: {best.path.name} from {best.source_location}\")\n",
    "    print(f\"   üìä Epoch {best.epoch}\", end=\"\")\n",
    "    if best.solvability is not None:\n",
    "        print(f\", validation solvability={best.solvability:.3f}\", end=\"\")\n",
    "    print()\n",
    "    \n",
    "    return best.path, best\n",
    "\n",
    "# Resume from checkpoint if exists (search across ALL sources)\n",
    "start_epoch = 0\n",
    "resume_history = []\n",
    "latest_ckpt_path, latest_ckpt_info = find_latest_checkpoint_multi_source(\n",
    "    checkpoint_patterns=['final_model.pth', 'best_model.pth', 'checkpoint_*.pth'],\n",
    "    required_keys=['epoch', 'diffusion_state_dict', 'ema_diffusion_state_dict']\n",
    ")\n",
    "\n",
    "if latest_ckpt_path is not None:\n",
    "    try:\n",
    "        # Copy to working directory if from input dataset\n",
    "        if IS_KAGGLE and latest_ckpt_info.source_type != 'working':\n",
    "            working_ckpt_path = copy_checkpoint_to_working(latest_ckpt_path, latest_ckpt_path.name)\n",
    "            print(f\"üìã Copied to working directory for incremental saves\")\n",
    "        else:\n",
    "            working_ckpt_path = latest_ckpt_path\n",
    "        \n",
    "        # Load the checkpoint into trainer\n",
    "        diff_trainer.load_checkpoint(str(latest_ckpt_path))\n",
    "        start_epoch = diff_trainer.epoch + 1\n",
    "        \n",
    "        # Try to load training history\n",
    "        history_path = CHECKPOINT_DIR / 'diffusion_history.json'\n",
    "        if history_path.exists():\n",
    "            with open(history_path, 'r') as f:\n",
    "                resume_history = json.load(f)\n",
    "                print(f\"üìä Loaded {len(resume_history)} epochs of training history\")\n",
    "        \n",
    "        print(f\"üîÑ Resuming from epoch {start_epoch}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error loading checkpoint {latest_ckpt_path.name}: {e}\")\n",
    "        print(\"   Starting fresh training\")\n",
    "        start_epoch = 0\n",
    "        resume_history = []\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  No existing diffusion checkpoint found - starting fresh training\")\n",
    "\n",
    "best_solv, history = 0.0, resume_history\n",
    "if resume_history:\n",
    "    # Find the best validation solvability from history\n",
    "    best_solv = max((h.get('val_solvability', 0) for h in resume_history), default=0.0)\n",
    "    print(f\"üìà Previous best validation solvability: {best_solv:.4f}\")\n",
    "    \n",
    "t0 = time.time()\n",
    "print(f\"\\n{'Epoch':>6} | {'Loss':>8} | {'Diffusion':>10} | {'Logic':>8} | {'Val Solv':>10} | {'Time':>6}\")\n",
    "print(\"‚îÄ\" * 65)\n",
    "\n",
    "for epoch in range(start_epoch, DIFFUSION_EPOCHS):\n",
    "    train_m = diff_trainer.train_epoch(train_loader)\n",
    "    val_m = diff_trainer.validate(val_loader, num_samples=4)\n",
    "    lr = diff_trainer.scheduler.get_last_lr()[0]\n",
    "\n",
    "    rec = {'epoch': epoch+1, **train_m, 'val_solvability': val_m['val_solvability'], 'lr': lr}\n",
    "    history.append(rec)\n",
    "\n",
    "    logic_flag = \"üîí\" if epoch < WARMUP_EPOCHS else \"‚úÖ\"\n",
    "    elapsed = time.time() - t0\n",
    "    print(f\"  {epoch+1:4d}   | {train_m['loss']:.4f}  | {train_m['diffusion_loss']:.6f}  | \"\n",
    "          f\"{train_m['logic_loss']:.4f}{logic_flag} | {val_m['val_solvability']:.4f}     | {elapsed/60:.1f}m\")\n",
    "\n",
    "    # Save periodic checkpoints\n",
    "    if (epoch+1) % SAVE_EVERY == 0:\n",
    "        ckpt_path = CHECKPOINT_DIR / f'checkpoint_{epoch+1:04d}.pth'\n",
    "        if COMPACT_CHECKPOINTS:\n",
    "            _save_minimal_checkpoint(ckpt_path, diff_trainer, rec)\n",
    "            _prune_checkpoints(CHECKPOINT_DIR, keep=MAX_KEEP_CHECKPOINTS)\n",
    "        else:\n",
    "            diff_trainer.save_checkpoint(str(ckpt_path), rec)\n",
    "\n",
    "    # Save best model\n",
    "    if val_m['val_solvability'] > best_solv:\n",
    "        best_solv = val_m['val_solvability']\n",
    "        best_path = CHECKPOINT_DIR / 'best_model.pth'\n",
    "        if COMPACT_CHECKPOINTS:\n",
    "            _save_minimal_checkpoint(best_path, diff_trainer, rec)\n",
    "        else:\n",
    "            diff_trainer.save_checkpoint(str(best_path), rec)\n",
    "\n",
    "# Save final checkpoint (compact if configured)\n",
    "final_path = CHECKPOINT_DIR / 'final_model.pth'\n",
    "if COMPACT_CHECKPOINTS:\n",
    "    _save_minimal_checkpoint(final_path, diff_trainer, history[-1] if history else None)\n",
    "else:\n",
    "    diff_trainer.save_checkpoint(str(final_path), history[-1] if history else None)\n",
    "print(f\"\\n‚úÖ Done! Best val solvability: {best_solv:.4f}, Time: {(time.time()-t0)/60:.1f} min\")\n",
    "\n",
    "# Save training history separately\n",
    "with open(CHECKPOINT_DIR / 'diffusion_history.json', 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "# Plot training curves\n",
    "if len(history) > 1:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    ep_x = [h['epoch'] for h in history]\n",
    "    \n",
    "    axes[0,0].plot(ep_x, [h['loss'] for h in history], 'b-')\n",
    "    axes[0,0].set_title('Total Loss'); axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[0,1].plot(ep_x, [h['diffusion_loss'] for h in history], 'r-', label='Diffusion')\n",
    "    axes[0,1].plot(ep_x, [h['logic_loss'] for h in history], 'g-', label='Logic')\n",
    "    axes[0,1].axvline(x=WARMUP_EPOCHS, color='gray', ls='--', alpha=0.5, label='Warmup end')\n",
    "    axes[0,1].set_title('Loss Components'); axes[0,1].legend(); axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1,0].plot(ep_x, [h['solvability'] for h in history], 'c-', label='Train')\n",
    "    axes[1,0].plot(ep_x, [h['val_solvability'] for h in history], 'm-', label='Val')\n",
    "    axes[1,0].set_title('Solvability'); axes[1,0].legend(); axes[1,0].set_ylim(0,1); axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1,1].plot(ep_x, [h['lr'] for h in history], 'k-')\n",
    "    axes[1,1].set_title('Learning Rate')\n",
    "    axes[1,1].set_yscale('log'); axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Stage 2: Diffusion Training', fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(str(OUTPUT_DIR / 'diffusion_curves.png'), dpi=150)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba1a304",
   "metadata": {},
   "source": [
    "---\n",
    "## üé® Generation & Visualization\n",
    "\n",
    "Generate sample dungeons using the trained model with graph-conditioned DDIM sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32a094e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 5: Generate Sample Dungeons\n",
    "# ============================================================================\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "NUM_SAMPLES = 4\n",
    "\n",
    "print(\"üé® GENERATING SAMPLE DUNGEONS\")\n",
    "\n",
    "# Load best checkpoint\n",
    "ckpt_path = CHECKPOINT_DIR / 'best_model.pth'\n",
    "if not ckpt_path.exists():\n",
    "    ckpt_path = CHECKPOINT_DIR / 'final_model.pth'\n",
    "\n",
    "gen_config = DiffusionTrainingConfig(\n",
    "    data_dir=str(DATA_DIR), batch_size=1, use_vglc=True,\n",
    "    vqvae_checkpoint=str(VQVAE_SAVE_PATH), latent_dim=64,\n",
    "    model_channels=128, context_dim=256, num_timesteps=1000,\n",
    "    schedule_type='cosine', device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    ")\n",
    "gen = DiffusionTrainer(gen_config)\n",
    "gen.load_checkpoint(str(ckpt_path))\n",
    "print(f\"‚úÖ Loaded model (epoch {gen.epoch})\")\n",
    "\n",
    "# Get real graph conditioning\n",
    "cond_loader = create_dataloader(str(DATA_DIR), batch_size=1, shuffle=True,\n",
    "    use_vglc=True, normalize=True, load_graphs=True)\n",
    "conditionings, shapes = [], []\n",
    "for batch_data in cond_loader:\n",
    "    if isinstance(batch_data, (list, tuple)) and len(batch_data) == 2:\n",
    "        imgs, graphs = batch_data\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        for g in graphs:\n",
    "            try: conditionings.append(gen._encode_graph_conditioning(g))\n",
    "            except: pass\n",
    "        shapes.append(gen.encode_to_latent(imgs).shape)\n",
    "    if len(conditionings) >= NUM_SAMPLES: break\n",
    "\n",
    "# Generate\n",
    "gen.ema_diffusion.eval()\n",
    "generated = []\n",
    "with torch.no_grad():\n",
    "    for i in range(NUM_SAMPLES):\n",
    "        c = conditionings[i % len(conditionings)]\n",
    "        z = gen.ema_diffusion.sample(c, shape=shapes[0])\n",
    "        logits = gen.decode_from_latent(z)\n",
    "        tile_ids = logits.argmax(dim=1).squeeze(0).cpu().numpy()\n",
    "        generated.append(tile_ids)\n",
    "        print(f\"  Sample {i+1}: {tile_ids.shape}, {len(np.unique(tile_ids))} tile types\")\n",
    "\n",
    "# Visualize\n",
    "TILE_COLORS = {\n",
    "    0: '#1a1a2e', 1: '#e8d5b7', 2: '#4a4a4a', 3: '#8b7355',\n",
    "    10: '#90EE90', 11: '#FFD700', 12: '#FF6347', 13: '#9370DB',\n",
    "    14: '#DC143C', 15: '#FFA500', 20: '#FF4444', 21: '#00FF00',\n",
    "    22: '#FFD700', 23: '#8B0000', 30: '#FFFF00', 31: '#FF69B4',\n",
    "    32: '#00CED1', 33: '#87CEEB', 40: '#4169E1', 41: '#6495ED',\n",
    "    42: '#DEB887', 43: '#DA70D6',\n",
    "}\n",
    "\n",
    "def grid_to_rgb(grid):\n",
    "    h, w = grid.shape\n",
    "    rgb = np.full((h, w, 3), 0.5, dtype=np.float32)\n",
    "    for tid, hex_c in TILE_COLORS.items():\n",
    "        mask = grid == tid\n",
    "        if mask.any():\n",
    "            r, g, b = mcolors.hex2color(hex_c)\n",
    "            rgb[mask] = [r, g, b]\n",
    "    return rgb\n",
    "\n",
    "fig, axes = plt.subplots(1, NUM_SAMPLES, figsize=(5*NUM_SAMPLES, 8))\n",
    "if NUM_SAMPLES == 1: axes = [axes]\n",
    "for i, (ax, grid) in enumerate(zip(axes, generated)):\n",
    "    ax.imshow(grid_to_rgb(grid), interpolation='nearest', aspect='auto')\n",
    "    ax.set_title(f'Dungeon {i+1}\\n{grid.shape[0]}x{grid.shape[1]}')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('H-MOLQD Generated Dungeons (Graph-Conditioned)', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(OUTPUT_DIR / 'generated_samples.png'), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "for i, grid in enumerate(generated):\n",
    "    np.save(str(OUTPUT_DIR / f'dungeon_{i+1}.npy'), grid)\n",
    "print(f\"üíæ Saved {NUM_SAMPLES} dungeons to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa6a985",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì¶ Summary & Download\n",
    "\n",
    "### Checkpoints Saved\n",
    "\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `checkpoints/vqvae_pretrained.pth` | VQ-VAE encoder/decoder (44 tiles, 512 codebook) |\n",
    "| `checkpoints/best_model.pth` | Best diffusion model (lowest val loss) |\n",
    "| `checkpoints/final_model.pth` | Final diffusion model (last epoch) |\n",
    "\n",
    "### Generated Outputs\n",
    "\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `outputs/generated_samples.png` | Visualization of generated dungeons |\n",
    "| `outputs/dungeon_*.npy` | Raw tile grids (NumPy arrays) |\n",
    "\n",
    "### Download Results (Kaggle)\n",
    "\n",
    "```python\n",
    "import shutil\n",
    "shutil.make_archive('/kaggle/working/hmolqd_results', 'zip', '/kaggle/working/hmolqd_outputs')\n",
    "```\n",
    "\n",
    "Then download `hmolqd_results.zip` from the **Output** tab.\n",
    "\n",
    "### Next Steps (Local)\n",
    "1. **WFC Refinement** (Block V): `python -m src.generation.wfc_refiner --input outputs/dungeon_1.npy`\n",
    "2. **Cognitive Validation** (Block VI): `python -m src.simulation.cognitive_validator --dungeon outputs/dungeon_1.npy`\n",
    "3. **Quality-Diversity Search**: Use MAP-Elites with trained models for diverse dungeon generation"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
